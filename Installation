This looks like a request to replicate a document. I can't generate the exact file for you to copy and paste, as I'm an AI assistant, not a file editor. However, I can provide the text from the document in a way that preserves its formatting, including headings, bullet points, and code blocks. This will make it easier for you to copy and paste it into a platform like GitHub.

-----

### Everything needed to run CDOCKER

-----

### **0. Installing needed tools for CDOCKER**

In order to use CDOCKER you will need to:

  * Create a conda environment capable of building CHARMM (and optionally pyCHARMM).
  * (Recommended for the paper's workflow) Install the **MMTSB ToolSet** for pose clustering; follow the MMTSB installation instructions.
  * Obtain CHARMM (free to academics and government labs) from **Academic CHARMM**.
  * Follow the directions below to build a conda environment capable of installing CHARMM/pyCHARMM.
  * Install CHARMM and (optionally) pyCHARMM.

Reference for what the paper actually used: Wu & Brooks (2022), Covalent Docking in CDOCKER, JCAMD.

**PubMed:** `https://pubmed.ncbi.nlm.nih.gov/35984589/`

#### **Create a conda environment**

You will need a base Anaconda/Miniconda installation.

Make a conda environment (you can also use `conda env create -f <env>.yml` in ยง2):

```
conda create -y -n <name_of_environment> python =3.9
```

`# Python only needed if you'll use pyCHARMM`

Activate this environment:

```
conda activate <name_of_environment>
```

-----

### **Install mamba as a faster conda:**

```
conda install -y -c conda-forge mamba
```

**(Optional, GPU) Install CUDA from NVIDIA for OpenMM GPU acceleration**
`# Common choices-pick ONE that matches your system driver (see table below)`

```
mamba install -y -c nvidia cuda
```

`#typically installs CUDA 12.1.1`

```
mamba install -y -c "nvidia/label/cuda-12.0.0" cuda
```

`# installs CUDA 12.0`

You can see available CUDA Toolkit packages on the NVIDIA channels. (Driver/Toolkit compatibility table below mirrors the example format.)

#### **Install needed packages to build CHARMM (and match the paper's CDOCKER workflow)**

Minimal + paper-faithful (no MPI, no FFTDOCK/BLADE flags):

```
mamba install -y -c conda-forge \
  gcc gxx gfortran make cmake binutils \
  openmm rdkit
```

  * `gcc/gfortran/make/binutils/cmake` - toolchain to compile CHARMM.
  * `OpenMM` enables CHARMM/OpenMM parallel simulated annealing used in the paper.
  * `RDKit` ligand conformer generation (paper used ETKDG).
  * You'll obtain CGenFF/ParamChem externally (not on conda) and install **MMTSB ToolSet** separately if you want the same clustering step.

#### **Note on CUDA Toolkit/Driver and Compiler Compatibilities**

Check the installed CUDA driver on your GPU node with:

```
nvidia-smi
```

Example (top of the output):

```
+--
+
| NVIDIA-SMI 525.85.05 Driver Version: 525.85.05 CUDA Version: 12.0 |
|---
+
+-
```

A driver `525.85.05` is compatible with `CUDA 12.0`, `GCC >=12.1`, or `Intel Compilers 2021.6`.

The following table:

| **Toolkit Version** | **Minimum Required Driver** | **Recommended GCC** | **Recommended Intel Compilers** |
| :--- | :--- | :--- | :--- |
| CUDA 12.1.x | $\\ge530.30.02$ | 12.2 | 2021.7 |
| CUDA 12.0.0 | $\\ge525.85.05$ | 12.1 | 2021.6 |
| CUDA 11.8.x | $\\ge520.61.05$ | 11 | 2021 |
| CUDA 11.7.x | $\\ge515.48.07$ | 11 | 2021 |
| CUDA 11.6.x | $\\ge510.47.03$ | 11 | 2021 |
| CUDA 11.5.x | $\\ge495.29.05$ | 11 | 2021 |
| CUDA 11.4.x | $\\ge470.82.01$ | 9.x | 19.1 |
| CUDA 11.3.x | $\\ge465.19.01$ | 9.x | 19.1 |
| CUDA 11.2.x | $\\ge460.32.03$ | 9.x | 19.1 |
| CUDA 11.1 | $\\ge455.32$ | 9.x | 19.1 |
| CUDA 11.0 | $\\ge450.51.06$ | 9.x | 19.1 |
| CUDA 10 | $\\ge440.33$ | 10.2 | 18.0 |
| CUDA 9 | $\\ge396.37$ | 4.8.5 | 17.0 |
| CUDA 8 | $\\ge375.26$ | 4.8.2 | 15, 16 |

-----

### **2. Building the CHARMM/pyCHARMM compatible environment with a YAML file**

**cdocker\_wcuda12.yml**

```yaml
name: cdocker_wcuda12 # Name of your conda environment
channels:
  - conda-forge
  - nvidia/label/cuda-12.0.0 # pin CUDA 12.0 exactly; change as needed
  # - nvidia/label/cuda-12.1.1 # alt CUDA line (comment/uncomment as appropriate)
dependencies:
  - python=3.9
  - mamba
  - cuda # brings in the CUDA toolkit for GPU OpenMM
  - ca-certificates
  - certifi
  - openssl
  # Toolchain
  - gcc
  - gxx
  - gfortran
  - make
  - cmake
  - binutils
  # Paper-faithful runtime
  - openmm # CHARMM/OpenMM parallel SA during docking
  - rdkit # conformer generation
  # (Optional analytics/vis-commented out to keep this lean)
  # - pandas
  # - mdtraj
  # - biopython
  # - py3dmol
  # - pymol-open-source
  # - jupyterlab
  # - jupytext
  # prefix: /path/to/.conda/envs/cdocker_wcuda12 # optional explicit path
```

Install from the YAML:

```
conda env create -f cdocker_wcuda12.yml
```

(You can edit this YAML to change CUDA versions by switching the NVIDIA channel line, as shown above.)

-----

### **3. CHARMM and pyCHARMM installation once conda environment is installed and active**

#### **Build CHARMM (CDOCKER-ready)**

Minimal CPU build (no MPI, no FFTDock/BLADE):

```
conda activate cdocker_wcuda12
cd <charmm_root>
mkdir -p build_charmm && cd build_charmm
```

`# If OpenMM is installed in this conda env, detection is usually automatic.`
`# If needed, you can help the build find it:`

```
export OPENMM_HOME="$CONDA_PREFIX"
```

`# Minimal CPU build for CDOCKER (no FFTDOCK/BLADE/MPI)`

```
../configure -u -p <charmm_install_path>
make -j <n> install
```

OpenMM-enabled build (recommended to match Wu & Brooks, 2022):

```
conda activate cdocker_wcuda12
cd <charmm_root>/build_charmm
```

`# Help configure find OpenMM from conda if necessary`

```
export OPENMM_HOME="$CONDA_PREFIX"
```

`# Build with OpenMM support, still without FFTDock/BLADE/MPI`

```
../configure --with-openmm -u -p <charmm_install_path>
make -j <n> install
```

  * `<charmm_root>` is the path to the CHARMM source tree.
  * `<charmm_install_path>` is where you want the CHARMM installation to live.
  * `<n>` is the number of build threads.

#### **Why no `--with-fftdock` or `--with-blade` here?**

Those are separate CHARMM features not required for Rigid CDOCKER; the paper did not rely on them. The recommended path is to enable OpenMM so you can use CHARMM/OpenMM parallel simulated annealing during docking.

#### **Build pyCHARMM (optional)**

```
conda activate cdocker_wcuda12
cd <charmm_root>/build_charmm
rm -rf * # Clean the build directory
```

`# As a shared library (for pyCHARMM); keep it lean and consistent with above`

```
export OPENMM_HOME="$CONDA_PREFIX"
../configure --as-library --without-mpi --with-openmm -u -p <pycharmm_install_path>
```

-----

### **make -j \<n\> install**

`# Install the Python bindings`

```
cd <charmm_root>
pip install "$(pwd)/tool/pycharmm"
```

`# Point pyCHARMM at your CHARMM lib`

```
export CHARMM_LIB_DIR="<pycharmm_install_path>/lib"
```

  * `<pycharmm_install_path>` is where you want the pyCHARMM-ready CHARMM library to reside.

#### **Final notes**

  * **MMTSB ToolSet** (for pose clustering) and **CGenFF/ParamChem** (for ligand topology/parameters) are outside this conda env; install/obtain them separately to replicate the paper's workflow.
  * If you only run CDOCKER from CHARMM scripts (no Python wrapper), you can omit Python/pyCHARMM entirely.
  * If you are CPU-only, skip the CUDA bits; OpenMM still works on CPU.
